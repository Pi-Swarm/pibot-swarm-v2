"""
๐ง Qwen2.5:0.5B - ุงููุณุฎุฉ ุงูุณุฑูุนุฉ ุฌุฏุงู
ูููุฐุฌ ุฎููู ููููุงู ุงูุจุณูุทุฉ ูุงูุณุฑูุนุฉ

ุงูุงุณุชุฎุฏุงู:
    from llm_connector_fast import QwenConnector
    connector = QwenConnector()  # ูุณุชุฎุฏู 0.5B ุชููุงุฆูุงู
"""

import urllib.request
import urllib.error
import json
from typing import Dict, List, Optional

# --- ุฅุนุฏุงุฏุงุช Ollama - ูููุฐุฌ ุฃุณุฑุน ---

OLLAMA_API = "http://localhost:11434"
MODEL_NAME = "qwen2.5:0.5b"  # ๐ ุฃุณุฑุน 3x ูู 1.5B

# ุฅุนุฏุงุฏุงุช ููุญุณููุฉ ููุณุฑุนุฉ
GENERATION_CONFIG = {
    "temperature": 0.5,      # ุฃุนูู ููููุงู ููุฅุจุฏุงุน
    "top_p": 0.95,
    "num_predict": 256,      # ุฑุฏูุฏ ุฃูุตุฑ = ุฃุณุฑุน
    "num_ctx": 2048,         # ุณูุงู ุฃูุตุฑ = ุฃุณุฑุน
    "num_thread": 4,         # ุนุฏุฏ ุงูุฃูููุฉ (ุนุฏูู ุญุณุจ CPU)
    "stop": ["</s>", "\n\n"]
}

class QwenConnector:
    """ููุตู ุณุฑูุน ูููููุฐุฌ ุงูุตุบูุฑ"""
    
    def __init__(self, model: str = MODEL_NAME, api_url: str = OLLAMA_API):
        self.model = model
        self.api_url = api_url
        self.base_url = f"{api_url}/api"
        print(f"๐ QwenFast: {model} ({MODEL_NAME})")
    
    def _check_connection(self) -> bool:
        try:
            req = urllib.request.Request(f"{self.api_url}/api/tags")
            with urllib.request.urlopen(req, timeout=3) as response:
                return response.status == 200
        except:
            return False
    
    def generate(self, prompt: str, context: Optional[Dict] = None, timeout_sec: int = 30) -> str:
        """
        ุชูููุฏ ุฑุฏ ุณุฑูุน
        
        Args:
            prompt: ุงูุณุคุงู
            context: ุณูุงู ุฅุถุงูู
            timeout_sec: ูููุฉ ุจุงูุซูุงูู (ุฃูุตุฑ = ุฃุณุฑุน ูุดูุงู)
        """
        if context:
            full_prompt = f"ุงูุณูุงู:\n{json.dumps(context, ensure_ascii=False)}\n\nุงูุณุคุงู: {prompt}"
        else:
            full_prompt = prompt
        
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": "ุฃูุช Pi bot ๐ฅงุ ูุณุงุนุฏ ุฃููู ููุดุจูุงุช."},
                {"role": "user", "content": full_prompt}
            ],
            "stream": False,
            "options": GENERATION_CONFIG
        }
        
        try:
            data = json.dumps(payload).encode('utf-8')
            req = urllib.request.Request(
                f"{self.base_url}/chat",
                data=data,
                headers={'Content-Type': 'application/json'},
                method='POST'
            )
            
            with urllib.request.urlopen(req, timeout=timeout_sec) as response:
                result = json.loads(response.read().decode('utf-8'))
                return result.get("message", {}).get("content", "ูุง ููุฌุฏ ุฑุฏ")
                
        except urllib.error.URLError as e:
            if "timed out" in str(e).lower():
                return f"โฑ๏ธ ูููุฉ ูุตูุฑุฉ ({timeout_sec}ุซ) - ุฌุฑูุจ ูููุฐุฌุงู ุฃุตุบุฑ ุฃู ุฒุฏ ุงููููุฉ"
            return f"โ ุฎุทุฃ: {str(e)}"
        except Exception as e:
            return f"โ ุฎุทุฃ: {str(e)}"
    
    def analyze_ports_fast(self, ports: List[int]) -> str:
        """ุชุญููู ุณุฑูุน ููููุงูุฐ - ุฑุฏ ุฎูุงู 10-15 ุซุงููุฉ"""
        prompt = f"ููุงูุฐ: {ports}. ููู ูููุฐ: ุงูุฎุทุฑ (HIGH/MED/LOW) + ุฎุฏูุฉ + ุชูุตูุฉ. ุฌุฏูู ูุฎุชุตุฑ."
        return self.generate(prompt, timeout_sec=20)
    
    def quick_decision(self, scenario: str) -> str:
        """ูุฑุงุฑ ุณุฑูุน - 5-10 ุซูุงูู"""
        prompt = f"ุณููุงุฑูู: {scenario}. ุงููุฑุงุฑ: (ุชูุตูุฉ ูุงุญุฏุฉ ูุญุฏุฏุฉ)"
        return self.generate(prompt, timeout_sec=15)

# --- ุงุฎุชุจุงุฑ ---

if __name__ == "__main__":
    print("๐ ุงุฎุชุจุงุฑ Qwen2.5:0.5B ุงูุณุฑูุน\n")
    connector = QwenConnector()
    
    if not connector._check_connection():
        print("โ Ollama ุบูุฑ ูุชุตู")
        exit(1)
    
    print("โ ูุชุตู - ุงุฎุชุจุงุฑ ุงูุชุญููู ุงูุณุฑูุน:")
    print("-" * 50)
    
    import time
    start = time.time()
    response = connector.analyze_ports_fast([22, 445, 80])
    elapsed = time.time() - start
    
    print(response)
    print(f"\nโฑ๏ธ ุงูููุช: {elapsed:.1f} ุซุงููุฉ")
    print("โ ุฅุฐุง ูุงู < 20ุซ โ ููุชุงุฒ!")
    print("โ๏ธ ุฅุฐุง ูุงู > 30ุซ โ ุงุณุชุฎุฏู ุงูููุงูุจ ุงูุฌุงูุฒุฉ ุจุฏูุงู ูู LLM")
