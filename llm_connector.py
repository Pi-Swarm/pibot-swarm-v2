"""
๐ง Qwen2.5:1.5B Integration Module
ุฑุจุท ูููุฐุฌ Qwen2.5:1.5B ูุน ุณุฑุจ Pi bot 2.0

ุจุฏูู ููุชุจุงุช ุฎุงุฑุฌูุฉ - ูุณุชุฎุฏู urllib ุงูููุงุณู

ุงูุงุณุชุฎุฏุงู:
    from llm_connector import QwenConnector
    
    connector = QwenConnector()
    response = connector.generate("ูุฑุญุจุงูุ ูู ุฃูุชุ")
"""

import urllib.request
import urllib.error
import json
from typing import Dict, List, Optional
from datetime import datetime

# --- ุฅุนุฏุงุฏุงุช Ollama ---

OLLAMA_API = "http://localhost:11434"
MODEL_NAME = "qwen2.5:1.5b"

# ุฅุนุฏุงุฏุงุช ุงูุชุญุณูู ูููููุฐุฌ ุงูุตุบูุฑ
GENERATION_CONFIG = {
    "temperature": 0.3,
    "top_p": 0.9,
    "num_predict": 512,
    "num_ctx": 4096,
    "stop": ["</s>", "User:", "\n\n"]
}

class QwenConnector:
    """
    ๐ค ููุตู Qwen2.5:1.5B ููุณุฑุจ
    
    ููุงุญุธุงุช ูููุฉ ูููููุฐุฌ ุงูุตุบูุฑ:
    1. ุงุณุชุฎุฏู Prompts ูุงุถุญุฉ ููุจุงุดุฑุฉ
    2. ุชุฌูุจ ุงูุฃุณุฆูุฉ ุงูููุชูุญุฉ ุงููุนูุฏุฉ
    3. ูุณูู ุงูููุงู ุงููุจูุฑุฉ ูููุงู ุตุบูุฑุฉ
    """
    
    def __init__(self, model: str = MODEL_NAME, api_url: str = OLLAMA_API):
        self.model = model
        self.api_url = api_url
        self.base_url = f"{api_url}/api"
        self.conversation_history: List[Dict] = []
        self.system_prompt = self._default_system_prompt()
        
        # ุงูุชุญูู ูู ุงุชุตุงู Ollama
        if not self._check_connection():
            print(f"โ๏ธ ุชุญุฐูุฑ: ูุง ูููู ุงูุงุชุตุงู ุจู Ollama ุนูู {api_url}")
            print("  ุชุฃูุฏ ูู ุชุดุบูู: ollama serve")
    
    def _check_connection(self) -> bool:
        """ุงูุชุญูู ูู ุฃู Ollama ูุนูู"""
        try:
            req = urllib.request.Request(f"{self.api_url}/api/tags", method='GET')
            with urllib.request.urlopen(req, timeout=5) as response:
                return response.status == 200
        except:
            return False
    
    def _default_system_prompt(self) -> str:
        """System Prompt ุงูุชุฑุงุถู ูู Pi bot"""
        return """ุฃูุช Pi bot ๐ฅงุ ูุณุงุนุฏ ุฃููู ุฐูู ูุชุฎุตุต ูู ุงููุญุต ุงูุฏูุงุนู ููุดุจูุงุช.

ุงููุจุงุฏุฆ:
1. ูู ุฏูููุงู ูู ุงูุชุญููู
2. ูุง ุชุณุชุบู ุงูุซุบุฑุงุชุ ููุท ุงูุชุดููุง
3. ูุฏูู ุชูุตูุงุช ูุงุจูุฉ ููุชูููุฐ
4. ุงุนูู ุถูู ุงููุทุงู ุงููุตุฑุญ ุจู ููุท

ุงูุชูุณูู:
- ุฅุฌุงุจุงุช ูุงุถุญุฉ ููุจุงุดุฑุฉ
- ุงุณุชุฎุฏู ุงูุฌุฏุงูู ุนูุฏ ุงูุญุงุฌุฉ
- ุฑูู ุงูุชูุตูุงุช ุจุงูุฃููููุฉ"""
    
    def set_system_prompt(self, prompt: str):
        """ุชุบููุฑ ุงูู System Prompt"""
        self.system_prompt = prompt
        print(f"โ ุชู ุชุญุฏูุซ System Prompt")
    
    def generate(
        self, 
        prompt: str, 
        context: Optional[Dict] = None,
        use_history: bool = False,
        **kwargs
    ) -> str:
        """
        ุชูููุฏ ุฑุฏ ูู ุงููููุฐุฌ
        
        Args:
            prompt: ุณุคุงู ุงููุณุชุฎุฏู
            context: ุณูุงู ุฅุถุงูู
            use_history: ุงุณุชุฎุฏุงู ุณุฌู ุงููุญุงุฏุซุฉ
            **kwargs: ุฅุนุฏุงุฏุงุช ุฅุถุงููุฉ
        
        Returns:
            str: ุฑุฏ ุงููููุฐุฌ
        """
        # ุฏูุฌ ุงูุณูุงู
        if context:
            full_prompt = self._format_with_context(prompt, context)
        else:
            full_prompt = prompt
        
        # ุจูุงุก ุงูุฑุณุงุฆู
        messages = [{"role": "system", "content": self.system_prompt}]
        
        if use_history:
            messages.extend(self.conversation_history[-10:])
        
        messages.append({"role": "user", "content": full_prompt})
        
        # ุฅุนุฏุงุฏ ุงูุทูุจ
        config = {**GENERATION_CONFIG, **kwargs}
        
        payload = {
            "model": self.model,
            "messages": messages,
            "stream": False,
            "options": config
        }
        
        try:
            data = json.dumps(payload).encode('utf-8')
            req = urllib.request.Request(
                f"{self.base_url}/chat",
                data=data,
                headers={'Content-Type': 'application/json'},
                method='POST'
            )
            
            with urllib.request.urlopen(req, timeout=60) as response:
                result = json.loads(response.read().decode('utf-8'))
                assistant_message = result.get("message", {}).get("content", "ูุง ููุฌุฏ ุฑุฏ")
                
                # ุญูุธ ูู ุงูุณุฌู
                if use_history:
                    self.conversation_history.append({"role": "user", "content": full_prompt})
                    self.conversation_history.append({"role": "assistant", "content": assistant_message})
                
                return assistant_message
                
        except urllib.error.URLError as e:
            if "refused" in str(e).lower():
                return "โ ูุง ูููู ุงูุงุชุตุงู ุจู Ollama - ุชุฃูุฏ ูู ุชุดุบูู: ollama serve"
            return f"โ ุฎุทุฃ ูู ุงูุงุชุตุงู: {str(e)}"
        except Exception as e:
            return f"โ ุฎุทุฃ ูู ุงููููุฐุฌ: {str(e)}"
    
    def _format_with_context(self, prompt: str, context: Dict) -> str:
        """ุชูุณูู ุงูุณุคุงู ูุน ุงูุณูุงู"""
        context_str = json.dumps(context, indent=2, ensure_ascii=False)
        return f"""ุงูุณูุงู:
{context_str}

ุงูุณุคุงู:
{prompt}
"""
    
    def analyze_ports(self, open_ports: List[int], target: str) -> str:
        """ุชุญููู ุงูููุงูุฐ ุงูููุชูุญุฉ"""
        prompt = f"""ูุฏููุง {len(open_ports)} ููุงูุฐ ููุชูุญุฉ ุนูู {target}:
{open_ports}

ูู ุจู:
1. ุชุญุฏูุฏ ุงูุฎุทุฑ ููู ูููุฐ (HIGH/MEDIUM/LOW)
2. ุฐูุฑ ุงูุฎุฏูุฉ ุงููุชููุนุฉ
3. ุงูุชุฑุงุญ ุฅุฌุฑุงุก ููู ูููุฐ

ุงูุฌุฏูู:
| ุงููููุฐ | ุงูุฎุฏูุฉ | ุงูุฎุทุฑ | ุงูุฅุฌุฑุงุก |
"""
        return self.generate(prompt)
    
    def generate_report_summary(self, scan_data: Dict) -> str:
        """ุชูููุฏ ููุฎุต ุชูููุฐู"""
        prompt = f"""ุจูุงูุงุช ุงููุญุต:
{json.dumps(scan_data, indent=2, ensure_ascii=False)}

ุงูุชุจ ููุฎุตุงู ุชูููุฐูุงู (3-5 ุฃุณุทุฑ) ูุดูู:
1. ูุณุชูู ุงูุฎุทุฑ ุงูุนุงู
2. ุฃูู 3 ุงูุชุดุงูุงุช
3. ุงูุฅุฌุฑุงุก ุงูุนุงุฌู ุงููุทููุจ
"""
        return self.generate(prompt)
    
    def clear_history(self):
        """ูุณุญ ุณุฌู ุงููุญุงุฏุซุฉ"""
        self.conversation_history = []
        print("โ ุชู ูุณุญ ุณุฌู ุงููุญุงุฏุซุฉ")

# --- ุฏูุงู ูุณุงุนุฏุฉ ---

def generate_response(prompt: str, **kwargs) -> str:
    """ุฏุงูุฉ ุณุฑูุนุฉ"""
    return QwenConnector().generate(prompt, **kwargs)

# --- ุงูุชุดุบูู ุงููุจุงุดุฑ ---

if __name__ == "__main__":
    print("๐ง ุงุฎุชุจุงุฑ Qwen2.5:1.5B ูุน Pi bot Swarm\n")
    
    connector = QwenConnector()
    
    # ุงุฎุชุจุงุฑ ุงูุงุชุตุงู
    print("=" * 50)
    if connector._check_connection():
        print("โ Ollama ูุชุตู - ุงููููุฐุฌ:", connector.model)
    else:
        print("โ Ollama ุบูุฑ ูุชุตู")
        print("\n๐ก ููุชุดุบูู:")
        print("   1. ุชุฃูุฏ ูู ุชุซุจูุช Ollama")
        print("   2. ุดุบูู: ollama serve")
        print("   3. ุชุฃูุฏ ูู ุงููููุฐุฌ: ollama pull qwen2.5:1.5b")
        print("   4. ุฃุนุฏ ุงูุงุฎุชุจุงุฑ")
        exit(1)
    
    # ุงุฎุชุจุงุฑ 1
    print("\n" + "=" * 50)
    print("๐ ุงูุงุฎุชุจุงุฑ 1: ุงุชุตุงู ุจุณูุท")
    print("=" * 50)
    response = connector.generate("ูุฑุญุจุงูุ ูู ุฃูุชุ")
    print(response)
    
    # ุงุฎุชุจุงุฑ 2
    print("\n" + "=" * 50)
    print("๐ ุงูุงุฎุชุจุงุฑ 2: ุชุญููู ููุงูุฐ")
    print("=" * 50)
    response = connector.analyze_ports([22, 445, 80], "192.168.122.1")
    print(response)
    
    # ุงุฎุชุจุงุฑ 3
    print("\n" + "=" * 50)
    print("โ ุงูุชูู ุงูุงุฎุชุจุงุฑ!")
    print("=" * 50)
